#!/usr/bin/env python3
#
# Convert the book into a website
#

from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple
import argparse
import re
import os

#############################################################################
# Book format
#############################################################################
#
# Basics:
# - the book is a collection of articles grouped into chapters.
# - any file ending in '.md' in the file tree is considered a source file.
# - the format is not Markdown directly. Each source file needs to be
#   preprocessed to be converted into one or more Markdown files with metadata.
# - within a chapter, some special structure can be imposed such as checking
#   that articles are ordered or can be ordered without cycles, don't
#   contain external links, etc.
# - Markdown files are the result of this build. They are rendered into HTML
#   using an external tool that does this well, Pandoc.
#
# Elements of implementation:
# - the Article type
# - the Chapter type
# - the preprocessor
#
# Structure of the source tree:
# - one folder per chapter
# - one or several source files per chapter folder
# - each article is named independently from the name of the source file
#
# Source file syntax:
#
#   *** eat-vegetables
#   title: Eat Vegetables!
#   ids: vegetable, vegetables
#
#   You should eat a variety of fruits and vegetables as part of every
#   meal.
#
#############################################################################

@dataclass
class Article:
    # main unique ID, does not require URL or HTML encoding
    # format: [a-z]([a-z-]*[a-z])?
    main_id: str = ""

    # properly capitalized title, should match ID closely
    title: str = ""

    # alternative unique IDs including main_id and lowercased title
    ids: Set[str] = field(default_factory=lambda: set())

    # article body
    markdown_lines: List[str] = field(default_factory=lambda: [])

    # data extracted from the body
    dependencies: List[str] = field(default_factory=lambda: [])


@dataclass
class Chapter:
    id_: str
    title: str
    markdown_introduction: str
    article_title_prefix: str
    articles: List[Article]
    compact_toc: bool


# Check that the articles are in an order that respects the dependencies
def check_dependencies(articles: List[Article], allow_cycles: bool = False) -> None:
    article: Article
    all_ids = { id_
                for article in articles
                for id_ in article.ids }
    previous_ids: Set[str] = set()
    for article in articles:
        main_id = article.main_id
        for id_ in article.ids:
            if id_ in previous_ids:
                raise Exception(f"duplicate article ID: '{id_}'")
        for dep in article.dependencies:
            if not dep in all_ids:
                raise Exception(f"article '{main_id}' depends on unknown article '{dep}'")
            if not allow_cycles and not dep in previous_ids:
                raise Exception(f"article '{main_id}' depends on later article '{dep}'")
        previous_ids.update(article.ids)


def make_synonym_map(articles: List[Article]) -> Dict[str, str]:
    return { id_: article.main_id
             for article in articles
             for id_ in article.ids }


ID_RE = re.compile(r"""
  ^
  \*\*\*\**     # 3 or more stars
  \s*           # whitespace
  ([a-z0-9-]+)  # ID (filename/URL); must contain only URL-safe characters
  \s*
  $
""", re.X)

FIELD_RE = re.compile(r"""
  ^
  ([a-z_]+)     # key
  \s*           # whitespace
  :
  \s*           # whitespace
  (.*)          # value
  \s*
  $
""", re.X)


def match_id(line: str) -> Optional[str]:
    match = ID_RE.match(line)
    if match:
        return match.group(1)
    else:
        return None


def parse_field(line: str) -> Optional[Tuple[str, str]]:
    match = FIELD_RE.match(line)
    if match:
        return (match.group(1), match.group(2))
    else:
        return None


SHORT_LINK_RE = re.compile(r"""
  \[([^\[\]\(\)]+)\]  # bracketed contents
  (?!\()              # not followed by an opening parenthesis
""", re.X)


LINK_RE = re.compile(r"""
  \[([^\[\]\(\)]+)\]  # [foo]
  \(([^\[\]\(\)]+)\)  # (bar)
""", re.X)


# [foo] -> [foo](foo)
#
def replace_short_links(line: str) -> str:
    return re.sub(SHORT_LINK_RE, r"[\1](\1)", line)


def resolve_link(synonym_map: Dict[str, str], link: str) -> str:
    key = link.lower()
    if key in synonym_map:
        return synonym_map[key]
    else:
        raise Exception(f"invalid link: '{link}'")


def resolve_link_synonyms(synonym_map: Dict[str, str], line: str) -> str:
    return re.sub(
        LINK_RE,
        lambda m: f"[{m.group(1)}]({resolve_link(synonym_map, m.group(2))})",
        line)


# [foo](bar) -> [foo](#bar)
#
def make_links_local(line: str) -> str:
    return re.sub(LINK_RE, r"[\1](#\2)", line)


# Extract bar from [foo](bar)
#
def extract_links(line: str) -> List[str]:
    return [ x.group(2).lower() for x in re.finditer(LINK_RE, line) ]


# Parse the article body and convert it to proper MarkDown.
# Return (body, dependencies). A dependency is a MarkDown link such as
# 'bar' in '[foo](bar)' or 'foo' in '[foo]'.
#
# Conversions:
#   [foo](bar) -> [foo](#bar)
#   [Foo] -> [Foo](#foo)
#   [foos] -> [foos](#foo)
#
def parse_article_body(synonym_map: Dict[str, str],
                       lines: List[str]) -> Tuple[List[str], List[str]]:
    md_lines: List[str] = []
    dependencies: List[str] = []
    for line in lines:
        md_line = replace_short_links(line)
        dependencies.extend(extract_links(md_line))
        md_line = resolve_link_synonyms(synonym_map, md_line)
        md_line = make_links_local(md_line)
        md_lines.append(md_line)
    return (md_lines, dependencies)


# Format:
# - head section containing metadata section followed by document body.
# - metadata section is made of lines of the form "name: value",
#   blank lines are skipped until reaching a line that's not of the
#   "key: value".
# - the body is anything that follows the metadata section.
#
# Sample input:
#
#   title: Hello World
#   Something something
#   something
#
# If in the future we need more metadata or more structured metadata
# field values, we should switch to a standard data format like JSON or YAML
# for the head section.
#
def parse_article(main_id: str, lines: List[str]) -> Article:
    stream = iter(lines)
    title = None
    ids = set()
    body = []
    # Parse head section
    try:
        while True:
            line = next(stream)
            field = parse_field(line)
            if field:
                key, value = field
                if key == '':
                    pass
                elif key == 'title':
                    title = value
                elif key == 'synonyms':
                    synonyms = [x.strip() for x in value.split(',')]
                    ids.update(synonyms)
                else:
                    raise Exception(f"{main_id}: unsupported field: {key}")
            else:
                body.append(line)
                break
    except StopIteration:
        pass
    if not title:
        raise Exception(f"{main_id}: missing title")
    # Parse body
    try:
        while True:
            body.append(next(stream))
    except StopIteration:
        pass

    ids.add(main_id)
    ids.add(title.lower())
    return Article(main_id=main_id,
                   title=title,
                   ids=ids,
                   # body needs further processing
                   markdown_lines=body)


# Check links, fix links, extract dependencies.
# This modifies the Article object in place.
#
# The synonym map is built beforehand from the list of articles and
# their synonyms.
#
def parse_article_phase2(synonym_map: Dict[str, str],
                         article: Article) -> None:
    md_body, dependencies = parse_article_body(synonym_map,
                                               article.markdown_lines)
    article.markdown_lines = md_body
    article.dependencies = dependencies


# Split a source file into articles with metadata.
# A second phase of parsing is done after this because it requires knowledge
# of all the article IDs and their synonyms.
#
def split_source_file(filepath: Path) -> List[Article]:
    articles: List[Article] = []
    current_id: str = ""
    current_lines: List[str] = []
    def close_article() -> None:
        # The current article so far was parsed into a slug (URL path)
        # and lines of text. They need to be parsed further into a header
        # and body.
        nonlocal articles, current_id, current_lines
        if current_id:
            article = parse_article(current_id, current_lines)
            if article.main_id:
                articles.append(article)
        current_id = ""
        current_lines = []
    with open(filepath) as file:
        for line in file:
            opt_id = match_id(line)
            if opt_id:
                # We found the beginning of a new article.
                # Finish with the current article:
                close_article()
                current_id = opt_id
            elif current_id:
                current_lines.append(line)
            # else discard lines (at the beginning of the file)
    close_article()
    return articles


# The second phase of articles parsing needs to come after this
#
def collect_articles_in_folder(folder: Path) -> List[Article]:
    return [
        article
        for name in os.listdir(folder)
        if os.path.isfile(folder / name)
        for article in split_source_file(folder / name)
    ]


def collect_articles(source_folder: Path, chapters: List[Chapter]) -> None:
    for chapter in chapters:
        chapter.articles = \
            collect_articles_in_folder(source_folder / chapter.id_)

    # Check for broken internal links
    all_articles: List[Article] = [
        article
        for chapter in CHAPTERS
        for article in chapter.articles
    ]
    synonym_map = make_synonym_map(all_articles)
    for article in all_articles:
        parse_article_phase2(synonym_map, article)

    # Check for cyclic dependencies in definitions
    check_dependencies(DEFINITIONS.articles)
    check_dependencies(all_articles, allow_cycles=True)


def export_html_table_of_contents(chapters: List[Chapter]) -> None:
    print(f"## Contents")
    for chapter in chapters:
        print(f"<div><a href='#{chapter.id_}' class='toc-chapter'>{chapter.title}</a></div>")
        print("<div class='toc-chapter-body'>")
        for article in chapter.articles:
            if chapter.compact_toc:
                print(f"<a href='#{article.main_id}' class='toc-article-compact'>{article.title}</a>")
            else:
                print(f"<div><a href='#{article.main_id}' class='toc-article'>{article.title}</a></div>")
        print("</div>")

def export_to_markdown_page(chapters: List[Chapter]) -> None:
    export_html_table_of_contents(chapters)
    for chapter in chapters:
        print(f"<div><a id='{chapter.id_}'></a></div>\n")
        print(f"## {chapter.title}\n")
        if chapter.markdown_introduction:
            print(f"{chapter.markdown_introduction}\n")
        for article in chapter.articles:
            print(f"<div><a id='{article.main_id}'/></div>\n")
            print(f"### <a href='#{article.main_id}'"
                  f" style='font-weight:normal; color: grey'>#</a> "
                  f"{chapter.article_title_prefix}{article.title}\n")
            print(f"{''.join(article.markdown_lines)}\n")


DEFINITIONS = Chapter(
    id_="definitions",
    title="Definitions",
    markdown_introduction="""
This a collection of definitions that are important to us. They're
listed such that no term is referenced before its definition. This guarantees
no circular definitions.
""",
    article_title_prefix="<span class='def-prefix'>def</span> ",
    articles=[],
    compact_toc=True,
)

INTRODUCTIONS = Chapter(
    id_="introductions",
    title="Introductions",
    markdown_introduction="",
    article_title_prefix="",
    articles=[],
    compact_toc=False,
)

CASE_STUDIES = Chapter(
    id_="case-studies",
    title="Case Studies",
    markdown_introduction="",
    article_title_prefix="",
    articles=[],
    compact_toc=False,
)

EXTENSIONS = Chapter(
    id_="extensions",
    title="Extensions",
    markdown_introduction="",
    article_title_prefix="",
    articles=[],
    compact_toc=False,
)

CHAPTERS: List[Chapter] = [
    DEFINITIONS, INTRODUCTIONS, CASE_STUDIES, EXTENSIONS
]


def main() -> None:
    parser = argparse.ArgumentParser(description='Generate the HTML book.')
    parser.add_argument("source_folder",
                        help="the folder containing the unformatted book")
    args = parser.parse_args()
    collect_articles(Path(args.source_folder), CHAPTERS)

    export_to_markdown_page(CHAPTERS)


main()
